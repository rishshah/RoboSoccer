SIMULATION_TIME = 3.8    
MOTION_CLIP = CWD + "/imitation/debug/hands_opposite_non_periodic.bvh"

    def generate_reward(self, state, time, is_fallen):
        target, sim = self.motion_clip.similarity(time - self.time, state, self.STATE_KEYS)
        reward = sim * 0.001
        if is_fallen:
            print('(generate_reward) fallen ', time-self.time)
            reward -= 500 * np.exp(-(time - self.time)/10)
        # print(reward)
        return np.array([target[s] for s in self.STATE_KEYS]), reward


NUM_THREADS = 2
os.environ["OMP_NUM_THREADS"] = str(NUM_THREADS)

env = Environment()
N_S = env.state_dim
N_A = env.action_dim

GAMMA = 1
MAX_EP = 6000
MAX_EP_STEP = 500
Z = 250 
learning_rate = 0.00002

    def __init__(self, s_dim, a_dim):
        super(Net, self).__init__()
        self.s_dim = s_dim
        self.a_dim = a_dim
        
        self.com1 = nn.Linear(s_dim, Z)
        self.com2 = nn.Linear(Z, Z)
        
        self.mu1 = nn.Linear(Z, Z)
        self.sigma1 = nn.Linear(Z, Z)
        self.mu = nn.Linear(Z, a_dim)
        self.sigma = nn.Linear(Z, a_dim)
        

        self.c1 = nn.Linear(s_dim, Z)
        self.c2 = nn.Linear(Z, Z)
        self.c3 = nn.Linear(Z, Z)
        self.v = nn.Linear(Z, 1)
        
        set_init([self.com1, self.com2, self.mu1, self.sigma1, self.mu, self.sigma, self.c1, self.c2, self.c3, self.v])
        self.distribution = torch.distributions.Normal

    def forward(self, x):
        com1 = F.relu6(self.com1(x))
        com2 = F.relu6(self.com2(com1))
        
        mu1 = F.relu6(self.mu1(com2))
        mu = self.mu(mu1)
        
        sigma1 = F.relu6(self.sigma1(com2))
        sigma = F.softplus(self.sigma(sigma1)) 

        c1 = F.relu6(self.c1(x))
        c2 = F.relu6(self.c2(c1))
        c3 = F.relu6(self.c3(c2))
        values = self.v(c3)
        
        return mu, sigma, values
