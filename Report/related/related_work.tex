\chapter{Related Work}

Numerous methods have been explored for training agents to walk using RL. Since our work involves a mix of animation and learning to train agents we focus on most closely related work in motion retargeting and RL.

\section{Training Physics Based Models}
[MacAlpine et al. 2012]\cite{AAAI12-MacAlpine} designed architecture for an omnidirectional walk to be used by a humanoid robot soccer agent acting in the RoboCup3D simulation environment. The walk is based on a double linear inverted pendulum model. Their walk engine chooses trajectory such that center of mass of the model swings over the stance foot. After finalizing the position of end effectors via this method, they use inverse kinematics to get positions for rest of the joints and use standard PD controller to calculate the low level torques to be applied to make the bot reach those joint orientations. This is a typical example of using physics-based models that rely on compact set of parameters tuned in order to achieve desired behaviors. The walk though effective, results in unnatural gaits. Also, this method cannot be extended to train robots other kind of complex skills without relying on human insight to implement task-specific strategies. Consequently, we explore RL approach to try to make the walk more human-like as well as to build a generic framework to teach various skills to soccer robot. The RL approach can also be more powerful in learning complex skills easily without much manual parameter tuning. 

\section{Reinforcement Learning}
The introduction of deep neural network models for Reinforcement Learning has given rise to simulated agents that can perform a diverse array of challenging tasks. It offers one of the most general framework to take traditional robotics towards true autonomy and versatility. However, applying RL to high dimensional movement
systems like humanoid robots remains a challenging problem. [Peters et al. 2003]\cite{peters2003reinforcement} discuss merits of various approaches for use of RL in robotics. Policy gradient methods have emerged as an algorithm of choice for many continuous control problems [Sutton et al. 1998]\cite{Sutton:1998:IRL:551283}. However, the resulting behavior learnt through these methods appears less natural than their more manually engineered counterparts. One of the possible reasons is the difficulty in specifying reward functions for a natural movement. Crafting reward functions tailored to specific simulation task requires a substantial degree of human insight. [Xue Bin Peng et al. 2018]\cite{Peng:2018:DED:3197517.3201311} came up with innovative solution to integrate human motion clip data with the reward function for RL agent. Giving higher rewards for mimicking the human motion clip makes the policy/behavior learnt by agent more natural (human-like). We try to adapt this idea to our soccer framework to recreate the results obtained by [Xue Bin Peng et al. 2018]\cite{Peng:2018:DED:3197517.3201311} on our robot hierarchy. 

\section{Maintaining Balance and Stability}
Many skills (like squats and walking) require robot to learn to balance properly throughout the simulation. For Nao \cite{usermanual} robot, this in itself a challenging task. [Tutsoy et al. 2017]\cite{tutsoy2017learning} have tried RL approach along with complete symbolic inverse kinematics just to balance the full lower body of Nao agent. We incorporate some of their ideas in our RL task formulation just to make sure that robot learns to both imitate the motion and maintain its balance without falling. [Xiaochen et al. 2017]\cite{laisurvey} introduced standard stability criteria and provided some common balance strategies. We try to incorporate some of these conditions while designing our reward function. [Yang et al. 2017] \cite{yang2017emergence} also tried to make use of deep RL strategies to learn specific small joint movements on humanoid robots like active push-off of ankles and maintaining the center of mass inside the support polygon to ensure stability. We use some of their ideas to design state space and reward functions to ensure balance.       

\section{Motion Retargeting}
For tailoring motion clips to our bot's skeleton hierarchy we need to solve problem of Motion retargeting. It is essentially transfer of motion (i.e. joint orientations for all frames) from one character hierarchy to another. [Gleicher et al. 1998]\cite{Gleicher:1998:RMN:280814.280820} presents a technique for retargeting motion focusing primarily on adapting the motion of one figure to another with identical structure but different segment lengths. The algorithm is not just to naively copying the angles from one joint to other but also to use inverse kinematics to satisfy end effector constraints. Most other retargeting approaches model constraints to make sure the final animated clip looks natural and similar to original. [Al Borno et al. 2018] \cite{al2018robust} explores the space of robust physics based motion retargeting. They optimize a physical model to approximate an old observed trajectory for the new skeleton.  