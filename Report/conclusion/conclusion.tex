\chapter{Conclusion and Future Work}

Clearly, our method shows that RL can be used to learn skills easily with the help of motion clip rewards.
For retargeted motions, the success is limited to motions where balancing is not required (like in hand wave motion) but for motions where we require balancing the robot struggles with handling balancing simultaneously with mimicking the motion clip. So if we could capture better motion clips tailor made for our nao agent we can surely make it able to not only walk but perform much more complex skills. 
\\\\
An important idea from [Xue Bin Peng et al. 2018] \cite{Peng:2018:DED:3197517.3201311} could possibly make this RL-motion clip reward system work. The idea is to start the simulation from not just the initial pose but also arbitrary in between poses. This would clearly help the robot learn later parts of simulation equally well. Currently, without this method learning longer (more than 3-4 sec) motion clips is difficult and requires a lot more episodes. We tried to implement this idea in our project but getting robot in stable initial pose is in itself a challenging task and would require a lot of manual effort and human insight. Extensions of this work could try to implement this random restart method and see if it actually improves the results or not.
\\\\
Furthermore, other target specific positional rewards could be used to get the robot to move along the ground(like described by [Xue Bin Peng et al. 2018]\cite{Peng:2018:DED:3197517.3201311}). After the agent learns a bunch of motions we can begin to teach it to switch between multiple motions at the right time which is very crucial during a soccer match.
\\\\
In conclusion, We tried an innovative and promising approach of combining animation with reinforcement learning applied to a completely different domain which can make soccer agent's behaviors more human-like and perhaps more effective. Unlike earlier approaches proposed, our approach doesn't require human intervention and creativity to optimize parameters for every particular type of behavior and agent can itself do what's best, provided we tune the reward functions in right manner once.